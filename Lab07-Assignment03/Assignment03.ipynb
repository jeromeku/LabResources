{
 "metadata": {
  "name": "",
  "signature": "sha256:2bc718d25031abf1c4026f7903ade6417daed199c410b13ba632b3f97f92c70a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Assignment 03\n",
      "\n",
      "**Due:** 2015-03-12, 11:59 PM, as an IPython notebook (with related files) submitted via your repo in the course GitHub organization.  Edit the provided Solutions03 notebook with your solutions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. Enhancing the *UnivariateBayesianInference* class\n",
      "\n",
      "Your repo has a `univariate_bayes.py` file that implements a `univariate_bayes` module with a single class, `UnivariateBayesianInference`; this code was introduced in Lab06.  This class is meant to be used as a *base class* that is inherited by a class written to solve univariate inference problems with a particular type of likelihood function and data.  Lab06 provided examples for binomial and Poisson cases.  For this problem, you will enhance the capabilities of `UnivariateBayesianInference`.\n",
      "\n",
      "Your repo also has a `poisson_binomial.py` file, also meant to be used as module, with two classes that subclass `UnivariateBayesianInference`:  `PoissonRateInference` and `BinomialInference`.  We discussed and used these classes in Lab06.  There we created instances of those classes in the same file that defined them; here, you'll import the classes from the module and work with them in your solutions notebook."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 1.1 (2 points):\n",
      "\n",
      "> Add two new methods to `UnivariateBayesianInference` in `univariate_bayes.py`:\n",
      "* `mean()`, computing the posterior mean of the value of the parameter by quadrature\n",
      "* `std()`, computing the posterior standard deviation by quadrature\n",
      "\n",
      "> Be sure to maintain docstrings:  revise the module docstring to note that you have revised the module's code, and provide docstrings for each new method."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 1.2 (2 points):\n",
      "\n",
      "> Test the methods by creating one instance each of `PoissonRateInference` and `BinomialInference` in your solution notebook (with input arguments of your choosing) and comparing your quadrature-based mean and standard deviation posterior summaries with analytical formulas presented in lectures.  Do the comparison by calculating the percentage difference between the quadrature-based and formula-based results and displaying the the results and percentage differences in the notebook."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. Inference with the Cauchy distribution\n",
      "\n",
      "In Lab06 and Problem 1 the `UnivariateBayesianInference` base class was used to numerically compute results for inferences that we can handle analytically.  But this base class can also be used for problems that are not analytically tractable.  In this problem, you'll use it to do inference with data modeled with a **Cauchy distribution**.\n",
      "\n",
      "The Cauchy distribution has an undefined mean and an infinite variance.  It is troublesome to work with in frequentist statistics.  Even it's maximum likelihood estimator has complicated sampling properties that pose not just computational challenges, but conceptual ones (the best frequentist methods require adopting the *conditional frequentist approach* briefly described in Lecture 5 when we discussed the likelihood principle).  In Bayesian inference, it poses no conceptual difficulties, but it must be handled numerically.\n",
      "\n",
      "The Cauchy distribution is known in physics as the Lorentzian distribution, where in certain circumstances it describes the profile of spectral lines, and the distribution of particle mass peaks in accelerator experiments.  It appears in problems where the ratio of two quantities with normal errors is of interest; when the quantities are uncorrelated with zero mean, the PDF for the ratio is a Cauchy distribution.  The Student's-$t$ distribution with 1 degree of freedom is a Cauchy distribution.  It also arises in geometric inference problems, as you will see in this problem.\n",
      "\n",
      "You can find basic information about the Cauchy distribution [on Wikipedia](http://en.wikipedia.org/wiki/Cauchy_distribution) and in the [NIST Engineering Statistics Handbook](http://www.itl.nist.gov/div898/handbook/eda/section3/eda3663.htm)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 2.1 (2 points):\n",
      "\n",
      "A small lump of weakly radioactive material is a distance $d$ behind a 1-dimensional sensor placed on a barrier, but at an unknown position, $x_0$, along the sensor.  The sensor records the locations of $N$ gamma rays emitted by the lump, denoted $x_i$ (for $i=1$ to $N$).\n",
      "\n",
      "> Assume the lump emits gamma rays isotropically.  Show that the PDF for the detected location of a single gamma ray is a Cauchy distribution with location parameter $x_0$ and scale parameter $d$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 2.2 (5 points):\n",
      "\n",
      "Implement and demonstrate inference for the Cauchy location parameter, $x_0$:\n",
      "\n",
      "> * Create a Python module `cauchy` with a class `CauchyLocationInference` that implements inference for the location of Cauchy-distributed data with a known scale parameter.\n",
      "* In the notebook, define a function with signature `cauchy_case(x0, d, N, plot=True)` that does the following:\n",
      "    1. Use `scipy.stats.cauchy` to simulate a dataset of size `N` from a Cauchy distribution with location and scale `x0` and `d`.\n",
      "    2. Create an instance of your `CauchyLocationInference` class for inferring `x0` with the simulated data.\n",
      "    3. If the `plot` argument is True, plot the PDF for `x0`.\n",
      "    4. If the `plot` argument is True, show the posterior mean for `x0` on the plot by computing the mean with the `mean()` method, evaluating the PDF at that value, and plotting a marker on the curve just plotted in step 3.\n",
      "    5. Return 2 (scalar) values: the value of the posterior mean, and the mean value of the samples in the simulated dataset.\n",
      "* Run the function 5 times, with the same arguments, to produce a single plot with 5 example posterior PDFs.  Use a small sample size (say, $N=5$).  Be sure to label all plot axes in this exercise, and feel free to adjust plot parameters (axis limits, line widths, etc.) to help communicate the results."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Problem 2.3 (2 points):\n",
      "\n",
      "Means of one kind or another (e.g., arithmetic or weighted) are often used to estimate parameters from a sample of measurements modeled with IID sampling uncertainties.  The **central limit theorem** (CLT) provides a motivation for this.  It shows that, for large sample sizes $N$, and when the sampling distribution has a finite variance, the sample mean has a PDF that converges to a normal distribution with a standard deviation that shrinks with sample size $\\propto 1/\\sqrt{N}$.\n",
      "\n",
      "The Cauchy distribution does *not* have a finite variance; as a consequence, the CLT does not hold for the sample mean.  Show this numerically:\n",
      "\n",
      "> * Run your `cauchy_case` function many times (say, 1000, but use fewer for debugging!), with `plot=False`, collecting the posterior means and the sample means in separate arrays.  Use a largish sample size, say, $N=50$ or $100$.\n",
      "* Use matplotlib's `hist` function to plot histograms of the posterior means and sample means.  Use the `normed` parameter to plot the histograms normalize as piecewise-constant PDFs.  You can use the `alpha` (opacity) argument to make regions of overlap easier to discern.  Feel free to change the number of bins from the default value.\n",
      "* Plot the Cauchy PDF for a single observation as a solid curve on the same plot.\n",
      "* Comment on what the plot reveals about the behavior of the sample mean."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}